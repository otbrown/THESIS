In this section we consider variational searches using Matrix Product States. We will first discuss the theoretical underpinnings of such a technique -- what we vary, and what we search for. I will then describe details pertaining to my own implementations of these techniques. There is no suggestion that these implementation details represent best practise, or are in any sense \emph{the right way} to perform these calculations. They are merely how \emph{I} approached the problem. All these implementations were written in and for MATLAB, and can at time of writing be found in repositories hosted at Ref~\cite{otb:githome}.
 
 \subsection{Theory}
It is well known that one can use the Rayleigh-Ritz Variational Technique to find an approximation to the lowest eigenvalue and corresponding eigenfunction of a Hermitian operator. Given a set of variational parameters upon which the eigenfunctions depend, one can move always to a lower eigenvalue, by minimising over one parameter at a time \cite{ArfWeb_RRVT, Gasiorowicz_RVT}. Consequently, we can find an approximation to the ground state of a system by minimising the expression,
\begin{equation}
E = \frac{\langle \psi (\bar{x}^{*}) | \hat{H} | \psi (\bar{x}) \rangle}{\langle \psi (\bar{x}^{*}) | \psi (\bar{x}) \rangle},
\label{eq:vs1-1}
\end{equation}
where E is the energy of the system, \(\hat{H}\) is a Hamiltonian, \(\psi\) is an approximation to the ground state, and \( \mathbf{x} \) is some set of \emph{variational parameters}. Equally, we can find an approximation to the stationary state of an open quantum system by minimising the expression,
\begin{equation}
\frac{\mathrm{d}\rho}{\mathrm{d}t} = \langle \rho(\mathbf{x}^{*}) | \hat{\mathcal{L}} | \rho(\mathbf{x}) \rangle,
\label{eq:vs1-2}
\end{equation}
where \(\hat{\mathcal{L}}\) is a Liouvillian, \(\rho\) is an approximation to the stationary state, and \(\mathbf{x}\) is again some set of variational parameters. For the purposes of this theoretical discussion we will focus on the ground state case as the two cases are very similar, but there are additional complexities in the stationary state search. A visual representaion of the variational search procedure is provided in \cref{fig:vs1-1}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{\figpath/var_space}
\caption{A visual representation of a variational search using matrix product states. The purple background represents the total state space of the system, and the green oval is the part of that state space that can be represented by a matrix product state of some finite dimension. The orange star represents our desired solution state, and it is inaccessible to the matrix product state space. The black circle is the initial matrix product state, the black star is the nearest matrix product state approximation to the solution state, and the black squares are states through which the matrix product state transitions on its way to the solution state. The black dashed line represents a variational step -- optimisation over one or more of the variational parameters. The transitional states may or may not have some physical meaning in the context of the variational search depending on the specifics of the system being investigated. In general, however, if one wishes to know \emph{how} a system reaches the solution state a time evolution method should be used, not a variational search.}
\label{fig:vs1-1}
\end{figure}

When using Matrix Product States the set of variational parameters we employ are the individual site tensors. We shall discuss the search procedure as prescribed by Ulrich Schollw\"{o}ck's excellent review article \cite{Schollwoeck11}. I begin my explanation by assuming that we have already some initial matrix product state, \(\Psi_{\mathrm{Init}}\) which is normalised, and has dimensions \(N \times \chi \times \chi \times d\), where \(N\) is the number of sites in the system, \(\chi\) is the maximal allowed matrix dimension, and \(d\) is the local state space dimension. Additionally I assume we have some observable we wish to minimise the expectation value of, with an operator \(\hat{O}\), which we may represent as a matrix product operator \(O^{[n]}\). First, we construct left and right `blocks' for each site in the system. The left block for some site \(n\) is a rank-3 tensor which contains the expectation of \(\hat{O}\) from the first site up to the site \(n-1\). The right block for some site \(n\) is a rank-3 tensor which contains the expectation of \(\hat{O}\) from last site through to the site \(n+1\). This is shown diagramatically in \cref{fig:vs1-2}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{\figpath/LR_blocks}
\caption{A tensor network diagram for a system which has been partially contracted in order to form left and right blocks, \(L^{[n]}\) and \(R^{[n]}\). The upper red dot here is a tensor for the site \(n\), \(A^{[n]}\), and the lower red dot is its conjugate, \(A^{\dagger [n]}\). The blue square is the mpo tensor \(O^{[n]}\) of some observable with an operator \(\hat{O}\). The black lines represent tensor indices which can be contracted over. If this contraction is completed it will be equivalent to a cointraction over the full system, and the result will be the expectation value \(\langle \Psi | \hat{O} | \Psi \rangle \).}
\label{fig:vs1-2}
\end{figure}

The first site left block tensor \(L^{[1]}\) is just the scalar \(1\), as there are obviously no sites before the first. The second left block tensor \(L^{[2]}\) is then found by performing the contraction procedure,
\begin{align}
L^{[2]}_{r^{\prime}, c, q} &= \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} L^{[1]}_{c^{\prime}, r, p} A^{[1] \sigma}_{r, c} \right) \right), \notag \\
&= \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} A^{[1] \sigma}_{r, c} \right) \right),
\label{eq:vs1-3}
\end{align}
where \(A^{[n]}\) is the matrix product state tensor for the site \(n\), \(\sigma\) indexes the local physical state, \(r\) and \(c\) (`row' and `column') index the local virtual dimensions, primed indices relate to the conjugate matrix product state tensor \(A^{\dagger [n]}\), and \(p\) and \(q\) index the virtual dimensions of the matrix product operator. The procedure continues from there, much as you might expect, by moving on to the third site and so on until the last site is reached. The general formula for \(L^{[n]}\) is,
\begin{equation} 
L^{[n]}_{r^{\prime}, c, q} = \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [n-1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[n-1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} L^{[n-1]}_{c^{\prime}, r, p} A^{[n-1] \sigma}_{r, c} \right) \right).
\label{eq:vs1-4}
\end{equation}

The procedure for forming the right block is naturally very similar, starting from the last site with \(R^{[N]} = 1\) and,
\begin{equation}
R^{[n]}_{c^{\prime}, r, p} = \sum_{\sigma^{\prime}, r^{\prime}} A^{\dagger [n+1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, q} O^{[n+1] \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{c} R^{[n+1]}_{r^{\prime}, c, q} A^{[n+1] \sigma}_{r, c} \right) \right).  
\label{eq:vs1-5}
\end{equation}
Once we have formed these left and right blocks at each site, we move on to the variational procedure proper. 

We will sweep backwards and forwards through the system, updating each site tensor to minimise the energy of the overall state. Referring back to \cref{eq:vs1-1} we can see that it can be minimised by being rephrased as an eigenvalue problem,
\begin{align}
\frac{\langle \psi(\mathbf{x}^{*}) | \hat{H} | \psi(\mathbf{x}) \rangle}{\langle \psi(\mathbf{x}^{*}) | \psi(\mathbf{x}) \rangle } &= E, \notag \\
\Rightarrow \langle \psi(\mathbf{x}^{*}) | \hat{H} | \psi(\mathbf{x}) \rangle &= E \langle \psi(\mathbf{x}^{*}) | \psi(\mathbf{x}) \rangle, \notag \\
\Rightarrow \frac{\mathrm{d}}{\mathrm{d}\langle \psi(\mathbf{x}^{*}) |} \left( \langle \psi(\mathbf{x}^{*}) | \hat{H} | \psi(\mathbf{x}) \rangle \right) &= \frac{\mathrm{d}}{\mathrm{d}\langle \psi(\mathbf{x}^{*}) |} \left(  E \langle \psi(\mathbf{x}^{*}) | \psi(\mathbf{x}) \rangle \right), \notag \\
\Rightarrow \hat{H} |\psi(\mathbf{x}) \rangle &= E | \psi(\mathbf{x}) \rangle,
\label{eq:vs1-6}
\end{align}
which of course is an expression of the time-independent Schr\"{o}dinger equation. If we could solve that for the many-body state \(| \psi (\mathbf{x}) \rangle\) then we would not need matrix product states at all. Unfortunately, we cannot, but what matrix product states allow us to do is to form an effective Hamiltonian for some particular \(| \psi(x) \rangle\), and instead solve the more limited eigenvalue problem,
\begin{equation}
\hat{H}_{\mathrm{eff}} |\psi(x) \rangle = E_{[x]} |\psi(x) \rangle,
\label{eq:vs1-7}
\end{equation}
from which we simply select \(|\psi(x) \rangle\) which corresponds to the lowest real value of \(E_{[x]}\). In our case \(|\psi(x) \rangle\) is \(A^{[n]}\), and \(\hat{H}_{\mathrm{eff}}\) is formed by the contraction of the \emph{environment} of \(A^{[n]}\) \cite{Orus14}. That is we calculate,
\begin{equation}
\hat{H}_{\mathrm{eff}}^{[n]} = \langle \psi(\tilde{\mathbf{x}}) | \hat{H} | \psi(\tilde{\mathbf{x}}) \rangle,
\label{eq:vs1-8}
\end{equation}
where \(|\psi(\tilde{\mathbf{x}}) \rangle \) is our matrix product state \emph{excluding the site \(n\)}. Such a contraction is shown diagramatically in \cref{fig:vs1-3}. Mathematically, the contraction is performed as,
\begin{equation}
\hat{H}^{[n]\, \mathrm{eff}}_{r,c,r^{\prime},c^{\prime},\sigma,\sigma^{\prime}} = \sum_{p,q} L^{[n]}_{r,r^{\prime},p} O^{[n] \sigma, \sigma^{\prime}}_{p,q} R^{[n]}_{c,c^{\prime},q},
\label{eq:vs1-9}
\end{equation}
which seems simple enough, and indeed would be except that we have an eigenvalue problem to solve. As such we require \(\hat{H}_{\mathrm{eff}}^{[n]}\) to be a matrix, not a rank-6 tensor. This can be accomplished by joining the indices corresponding to the matrix product state, and joining those of its conjugate to form a matrix \(\hat{H}^{[n]\, \mathrm{eff}}_{(\sigma ,r,c), (\sigma^{\prime},r^{\prime},c^{\prime})}\). Once this is achieved it is a simple matter of finding the eigenvector of \(\hat{H}^{[n]\, \mathrm{eff}}_{(\sigma ,r,c), (\sigma^{\prime},r^{\prime},c^{\prime})}\) with the lowest real eigenvalue. This eigenvector is the vectorised site tensor \(A^{[n]}_{\sigma, r, c}\), which we reshape to be \(A^{[n] \sigma}_{r,c}\) and update our matrix product state.

We do this on the first site in our system, and then re-normalise using an SVD or QR decomposition in order to make the site left-canonical. We then update our left block for the second site in the system, \(L^{[2]}\) using \cref{eq:vs1-4}. We are then ready to find an effective Hamiltonian for the second site and update it. This procedure repeats sweeping `right' through our system until we reach and update the \(N\)th site -- at this point we have updated every site in the system, but it is unlikely that our observable has converged after only one such sweep. The procedure for sweeping `left' through the system back to the first site is very similar, except when re-normalising we make our newly updated site right-canonical and then update the right block, \(R^{[n]}\). In this way we are always using the most up-to-date version of the system when we calculate the effective operator for a given site. The whole procedure repeats, sweeping left and right through the system until our chosen observable converges.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{\figpath/effH_diagram}
\caption{A diagrammatic representation of the contraction that must be performed in order to find the effective operator on some site \(n\). As usual the red circles represent matrix product state tensors, the blue squares represent some matrix product operator, and black lines are indices. The lines which reach into the gap left by the missing site \(n\) are indices which are left free, and will become the indices of the effective operator. As such, it can be seen that the effective operator will be a rank-6 tensor.}
\label{fig:vs1-3}
\end{figure}

 \subsection{Ground State Implementation}
 
 \subsubsection{Ground.m}
 
 \subsection{Stationary State Implementation}