In this section we consider variational searches using Matrix Product States. We will first discuss the theoretical underpinnings of such a technique -- what we vary, and what we search for. I will then describe details pertaining to my own implementations of these techniques. There is no suggestion that these implementation details represent best practise, or are in any sense \emph{the right way} to perform these calculations. They are merely how \emph{I} approached the problem. All these implementations were written in and for MATLAB, and can at time of writing be found in repositories hosted at Ref~\cite{otb:githome}.
 
 \subsection{Theory}
It is well known that one can use the Rayleigh-Ritz Variational Technique to find an approximation to the lowest eigenvalue and corresponding eigenfunction of a Hermitian operator. Given a set of variational parameters upon which the eigenfunctions depend, one can move always to a lower eigenvalue, by minimising over one parameter at a time \cite{ArfWeb_RRVT, Gasiorowicz_RVT}. Consequently, we can find an approximation to the ground state of a system by minimising the expression,
\begin{equation}
E = \frac{\langle \psi (\bar{x}^{*}) | \hat{H} | \psi (\bar{x}) \rangle}{\langle \psi (\bar{x}^{*}) | \psi (\bar{x}) \rangle},
\label{eq:vs1-1}
\end{equation}
where E is the energy of the system, \(\hat{H}\) is a Hamiltonian, \(\psi\) is an approximation to the ground state, and \( \bar{x} \) is some set of \emph{variational parameters}. Equally, we can find an approximation to the stationary state of an open quantum system by minimising the expression,
\begin{equation}
\frac{\mathrm{d}\rho}{\mathrm{d}t} = \langle \rho(\bar{x}^{*}) | \hat{\mathcal{L}} | \rho(\bar{x}) \rangle,
\label{eq:vs1-2}
\end{equation}
where \(\hat{\mathcal{L}}\) is a Liouvillian, \(\rho\) is an approximation to the stationary state, and \(\bar{x}\) is again some set of variational parameters. For the purposes of this theoretical discussion we will focus on the ground state case as the two cases are very similar, but there are additional complexities in the stationary state search. A visual representaion of the variational search procedure is provided in \cref{fig:vs1-1}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{\figpath/var_space}
\caption{A visual representation of a variational search using matrix product states. The purple background represents the total state space of the system, and the green oval is the part of that state space that can be represented by a matrix product state of some finite dimension. The orange star represents our desired solution state, and it is inaccessible to the matrix product state space. The black circle is the initial matrix product state, the black star is the nearest matrix product state approximation to the solution state, and the black squares are states through which the matrix product state transitions on its way to the solution state. The black dashed line represents a variational step -- optimisation over one or more of the variational parameters. The transitional states may or may not have some physical meaning in the context of the variational search depending on the specifics of the system being investigated. In general, however, if one wishes to know \emph{how} a system reaches the solution state a time evolution method should be used, not a variational search.}
\label{fig:vs1-1}
\end{figure}

When using Matrix Product States the set of variational parameters we employ are the individual site tensors. We shall discuss the search procedure as prescribed by Ulrich Schollw\"{o}ck's excellent review article \cite{Schollwoeck11}. I begin my explanation by assuming that we have already some initial matrix product state, \(\Psi_{\mathrm{Init}}\) which is normalised, and has dimensions \(N \times \chi \times \chi \times d\), where \(N\) is the number of sites in the system, \(\chi\) is the maximal allowed matrix dimension, and \(d\) is the local state space dimension. Additionally I assume we have some observable we wish to minimise the expectation value of, with an operator \(\hat{O}\), which we may represent as a matrix product operator \(O^{[n]}\). First, we construct left and right `blocks' for each site in the system. The left block for some site \(n\) is a rank-3 tensor which contains the expectation of \(\hat{O}\) from the first site up to the site \(n-1\). The right block for some site \(n\) is a rank-3 tensor which contains the expectation of \(\hat{O}\) from last site through to the site \(n+1\). This is shown diagramatically in \cref{fig:vs1-2}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{\figpath/LR_blocks}
\caption{A tensor network diagram for a system which has been partially contracted in order to form left and right blocks, \(L^{[n]}\) and \(R^{[n]}\). The upper red dot here is a tensor for the site \(n\), \(A^{[n]}\), and the lower red dot is its conjugate, \(A^{\dagger [n]}\). The blue square is the mpo tensor \(O^{[n]}\) of some observable with an operator \(\hat{O}\). The black lines represent tensor indices which can be contracted over. If this contraction is completed it will be equivalent to a cointraction over the full system, and the result will be the expectation value \(\langle \Psi | \hat{O} | \Psi \rangle \).}
\label{fig:vs1-2}
\end{figure}

The first site left block tensor \(L^{[1]}\) is just the scalar \(1\), as there are obviously no sites before the first. The second left block tensor \(L^{[2]}\) is then found by performing the contraction procedure,
\begin{align}
L^{[2]}_{r^{\prime}, c, q} &= \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} L^{[1]}_{c^{\prime}, r, p} A^{[1] \sigma}_{r, c} \right) \right), \notag \\
&= \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} A^{[1] \sigma}_{r, c} \right) \right),
\label{eq:vs1-3}
\end{align}
where \(A^{[n]}\) is the matrix product state tensor for the site \(n\), \(\sigma\) indexes the local physical state, \(r\) and \(c\) (`row' and `column') index the local virtual dimensions, primed indices relate to the conjugate matrix product state tensor \(A^{\dagger [n]}\), and \(p\) and \(q\) index the virtual dimensions of the matrix product operator. The procedure continues from there, much as you might expect, by moving on to the third site and so on until the last site is reached. The general formula for \(L^{[n]}\) is,
\begin{equation} 
L^{[n]}_{r^{\prime}, c, q} = \sum_{\sigma^{\prime}, c^{\prime}} A^{\dagger [n-1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, p} O^{[n-1]  \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{r} L^{[n-1]}_{c^{\prime}, r, p} A^{[n-1] \sigma}_{r, c} \right) \right).
\label{eq:vs1-4}
\end{equation}

The procedure for forming the right block is naturally very similar, starting from the last site with \(R^{[N]} = 1\) and,
\begin{equation}
R^{[n]}_{c^{\prime}, r, p} = \sum_{\sigma^{\prime}, r^{\prime}} A^{\dagger [n+1] \sigma^{\prime}}_{r^{\prime}, c^{\prime}} \left( \sum_{\sigma, q} O^{[n+1] \sigma, \sigma^{\prime}}_{p, q} \left( \sum_{c} R^{[n+1]}_{r^{\prime}, c, q} A^{[n+1] \sigma}_{r, c} \right) \right).  
\label{eq:vs1-5}
\end{equation}

 \subsection{Ground State Implementation}
 
 \subsection{Stationary State Implementation}